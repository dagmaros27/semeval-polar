{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "im_ziYOb35ln",
        "outputId": "eea78dfb-4232-4eef-c391-0df23c2218e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Archive:  /content/drive/My Drive/dev_phase.zip\n",
            "   creating: /content/subtask1/\n",
            "   creating: /content/subtask1/dev/\n",
            "  inflating: /content/subtask1/dev/nep.csv  \n",
            "  inflating: /content/subtask1/dev/ita.csv  \n",
            "  inflating: /content/subtask1/dev/pol.csv  \n",
            "  inflating: /content/subtask1/dev/rus.csv  \n",
            "  inflating: /content/subtask1/dev/tel.csv  \n",
            "  inflating: /content/subtask1/dev/hin.csv  \n",
            "  inflating: /content/subtask1/dev/hau.csv  \n",
            "  inflating: /content/subtask1/dev/pan.csv  \n",
            "  inflating: /content/subtask1/dev/ori.csv  \n",
            "  inflating: /content/subtask1/dev/spa.csv  \n",
            "  inflating: /content/subtask1/dev/deu.csv  \n",
            "  inflating: /content/subtask1/dev/fas.csv  \n",
            "  inflating: /content/subtask1/dev/arb.csv  \n",
            "  inflating: /content/subtask1/dev/ben.csv  \n",
            "  inflating: /content/subtask1/dev/amh.csv  \n",
            "  inflating: /content/subtask1/dev/khm.csv  \n",
            "  inflating: /content/subtask1/dev/tur.csv  \n",
            "  inflating: /content/subtask1/dev/zho.csv  \n",
            "  inflating: /content/subtask1/dev/eng.csv  \n",
            "  inflating: /content/subtask1/dev/swa.csv  \n",
            "  inflating: /content/subtask1/dev/urd.csv  \n",
            "  inflating: /content/subtask1/dev/mya.csv  \n",
            "   creating: /content/subtask1/train/\n",
            "  inflating: /content/subtask1/train/nep.csv  \n",
            "  inflating: /content/subtask1/train/pol.csv  \n",
            "  inflating: /content/subtask1/train/rus.csv  \n",
            "  inflating: /content/subtask1/train/ita.csv  \n",
            "  inflating: /content/subtask1/train/hin.csv  \n",
            "  inflating: /content/subtask1/train/tel.csv  \n",
            "  inflating: /content/subtask1/train/fas.csv  \n",
            "  inflating: /content/subtask1/train/deu.csv  \n",
            "  inflating: /content/subtask1/train/hau.csv  \n",
            "  inflating: /content/subtask1/train/pan.csv  \n",
            "  inflating: /content/subtask1/train/ori.csv  \n",
            "  inflating: /content/subtask1/train/spa.csv  \n",
            "  inflating: /content/subtask1/train/arb.csv  \n",
            "  inflating: /content/subtask1/train/khm.csv  \n",
            "  inflating: /content/subtask1/train/tur.csv  \n",
            "  inflating: /content/subtask1/train/zho.csv  \n",
            "  inflating: /content/subtask1/train/amh.csv  \n",
            "  inflating: /content/subtask1/train/ben.csv  \n",
            "  inflating: /content/subtask1/train/swa.csv  \n",
            "  inflating: /content/subtask1/train/urd.csv  \n",
            "  inflating: /content/subtask1/train/eng.csv  \n",
            "  inflating: /content/subtask1/train/mya.csv  \n",
            "   creating: /content/subtask2/\n",
            "   creating: /content/subtask2/train/\n",
            "  inflating: /content/subtask2/train/nep.csv  \n",
            "  inflating: /content/subtask2/train/ita.csv  \n",
            "  inflating: /content/subtask2/train/rus.csv  \n",
            "  inflating: /content/subtask2/train/pol.csv  \n",
            "  inflating: /content/subtask2/train/hin.csv  \n",
            "  inflating: /content/subtask2/train/tel.csv  \n",
            "  inflating: /content/subtask2/train/deu.csv  \n",
            "  inflating: /content/subtask2/train/fas.csv  \n",
            "  inflating: /content/subtask2/train/pan.csv  \n",
            "  inflating: /content/subtask2/train/hau.csv  \n",
            "  inflating: /content/subtask2/train/spa.csv  \n",
            "  inflating: /content/subtask2/train/ori.csv  \n",
            "  inflating: /content/subtask2/train/arb.csv  \n",
            "  inflating: /content/subtask2/train/amh.csv  \n",
            "  inflating: /content/subtask2/train/zho.csv  \n",
            "  inflating: /content/subtask2/train/tur.csv  \n",
            "  inflating: /content/subtask2/train/khm.csv  \n",
            "  inflating: /content/subtask2/train/ben.csv  \n",
            "  inflating: /content/subtask2/train/swa.csv  \n",
            "  inflating: /content/subtask2/train/urd.csv  \n",
            "  inflating: /content/subtask2/train/eng.csv  \n",
            "  inflating: /content/subtask2/train/mya.csv  \n",
            "   creating: /content/subtask2/dev/\n",
            "  inflating: /content/subtask2/dev/pol.csv  \n",
            "  inflating: /content/subtask2/dev/rus.csv  \n",
            "  inflating: /content/subtask2/dev/ita.csv  \n",
            "  inflating: /content/subtask2/dev/nep.csv  \n",
            "  inflating: /content/subtask2/dev/fas.csv  \n",
            "  inflating: /content/subtask2/dev/deu.csv  \n",
            "  inflating: /content/subtask2/dev/spa.csv  \n",
            "  inflating: /content/subtask2/dev/ori.csv  \n",
            "  inflating: /content/subtask2/dev/pan.csv  \n",
            "  inflating: /content/subtask2/dev/hau.csv  \n",
            "  inflating: /content/subtask2/dev/hin.csv  \n",
            "  inflating: /content/subtask2/dev/tel.csv  \n",
            "  inflating: /content/subtask2/dev/tur.csv  \n",
            "  inflating: /content/subtask2/dev/zho.csv  \n",
            "  inflating: /content/subtask2/dev/khm.csv  \n",
            "  inflating: /content/subtask2/dev/amh.csv  \n",
            "  inflating: /content/subtask2/dev/ben.csv  \n",
            "  inflating: /content/subtask2/dev/arb.csv  \n",
            "  inflating: /content/subtask2/dev/mya.csv  \n",
            "  inflating: /content/subtask2/dev/urd.csv  \n",
            "  inflating: /content/subtask2/dev/swa.csv  \n",
            "  inflating: /content/subtask2/dev/eng.csv  \n",
            "   creating: /content/subtask3/\n",
            "   creating: /content/subtask3/dev/\n",
            "  inflating: /content/subtask3/dev/eng.csv  \n",
            "  inflating: /content/subtask3/dev/urd.csv  \n",
            "  inflating: /content/subtask3/dev/swa.csv  \n",
            "  inflating: /content/subtask3/dev/arb.csv  \n",
            "  inflating: /content/subtask3/dev/ben.csv  \n",
            "  inflating: /content/subtask3/dev/amh.csv  \n",
            "  inflating: /content/subtask3/dev/tur.csv  \n",
            "  inflating: /content/subtask3/dev/zho.csv  \n",
            "  inflating: /content/subtask3/dev/khm.csv  \n",
            "  inflating: /content/subtask3/dev/tel.csv  \n",
            "  inflating: /content/subtask3/dev/hin.csv  \n",
            "  inflating: /content/subtask3/dev/spa.csv  \n",
            "  inflating: /content/subtask3/dev/ori.csv  \n",
            "  inflating: /content/subtask3/dev/pan.csv  \n",
            "  inflating: /content/subtask3/dev/hau.csv  \n",
            "  inflating: /content/subtask3/dev/deu.csv  \n",
            "  inflating: /content/subtask3/dev/fas.csv  \n",
            "  inflating: /content/subtask3/dev/nep.csv  \n",
            "   creating: /content/subtask3/train/\n",
            "  inflating: /content/subtask3/train/amh.csv  \n",
            "  inflating: /content/subtask3/train/zho.csv  \n",
            "  inflating: /content/subtask3/train/tur.csv  \n",
            "  inflating: /content/subtask3/train/khm.csv  \n",
            "  inflating: /content/subtask3/train/ben.csv  \n",
            "  inflating: /content/subtask3/train/arb.csv  \n",
            "  inflating: /content/subtask3/train/swa.csv  \n",
            "  inflating: /content/subtask3/train/urd.csv  \n",
            "  inflating: /content/subtask3/train/eng.csv  \n",
            "  inflating: /content/subtask3/train/nep.csv  \n",
            "  inflating: /content/subtask3/train/deu.csv  \n",
            "  inflating: /content/subtask3/train/fas.csv  \n",
            "  inflating: /content/subtask3/train/pan.csv  \n",
            "  inflating: /content/subtask3/train/hau.csv  \n",
            "  inflating: /content/subtask3/train/spa.csv  \n",
            "  inflating: /content/subtask3/train/ori.csv  \n",
            "  inflating: /content/subtask3/train/hin.csv  \n",
            "  inflating: /content/subtask3/train/tel.csv  \n"
          ]
        }
      ],
      "source": [
        "# Only if using colab\n",
        "# Mount drive and load the dataset\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "zip_path = \"/content/drive/My Drive/dev_phase.zip\"\n",
        "!unzip \"$zip_path\" -d /content/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "tGO4-umeLwW_",
        "outputId": "b18131b6-027e-46b0-d5ad-623ab4bb41ff"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/notebook/notebookapp.py:191: SyntaxWarning: invalid escape sequence '\\/'\n",
            "  | |_| | '_ \\/ _` / _` |  _/ -_)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score, accuracy_score, classification_report\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModel,\n",
        "    Trainer,\n",
        "    TrainingArguments,\n",
        "    DataCollatorWithPadding\n",
        ")\n",
        "from torch.utils.data import Dataset\n",
        "import wandb\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "# Disable wandb\n",
        "wandb.init(mode=\"disabled\")\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {device}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJwph4w8OTbu"
      },
      "outputs": [],
      "source": [
        "LANGUAGE = 'swa'  # Change to 'eng', 'swa' or 'amh'\n",
        "\n",
        "MODELS_CONFIG = {\n",
        "    'eng': [\n",
        "        'cardiffnlp/twitter-roberta-base-hate-latest',\n",
        "        'microsoft/deberta-v3-base',\n",
        "        'FacebookAI/xlm-roberta-base'\n",
        "    ],\n",
        "    'swa': [\n",
        "        'cardiffnlp/twitter-roberta-base-hate-latest',\n",
        "        'Davlan/afro-xlmr-base'\n",
        "    ],\n",
        "    'amh': [\n",
        "        'cardiffnlp/twitter-roberta-base-hate-latest',\n",
        "        'Davlan/afro-xlmr-base'\n",
        "    ]\n",
        "}\n",
        "\n",
        "NUM_EPOCHS = 5\n",
        "BATCH_SIZE = 16\n",
        "LEARNING_RATE = 2e-5\n",
        "MAX_LENGTH = 256\n",
        "DROPOUT_RATE = 0.2\n",
        "\n",
        "RESULTS_CSV = 'results_subtask1_simple.csv'\n",
        "PREDICTIONS_DIR = 'predictions_subtask1_simple'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZBkNgFsrOUqd"
      },
      "outputs": [],
      "source": [
        "class LabelAwareDebertaBinary(nn.Module):\n",
        "    \"\"\"\n",
        "    Custom model for Binary Classification (0: Polarized, 1: Not Polarized)\n",
        "    \"\"\"\n",
        "    def __init__(self, model_name, num_labels=2, dropout_rate=0.2):\n",
        "        super(LabelAwareDebertaBinary, self).__init__()\n",
        "\n",
        "        # Load base model\n",
        "        self.deberta = AutoModel.from_pretrained(model_name)\n",
        "        hidden_size = self.deberta.config.hidden_size\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "        self.classifier = nn.Linear(hidden_size, num_labels)\n",
        "\n",
        "        # Initialize weights\n",
        "        nn.init.xavier_uniform_(self.classifier.weight)\n",
        "        nn.init.zeros_(self.classifier.bias)\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, labels=None):\n",
        "        outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.last_hidden_state[:, 0, :]\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss_fct = nn.CrossEntropyLoss(weight=self.class_weights if hasattr(self, 'class_weights') else None)\n",
        "            loss = loss_fct(logits, labels.view(-1))\n",
        "\n",
        "        return {'loss': loss, 'logits': logits}\n",
        "\n",
        "\n",
        "class PolarizationDatasetBinary(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=256):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.instruction = \"Classify if this text is polarizing (0) or not polarized (1). Text: \"\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "\n",
        "        try:\n",
        "            label = int(self.labels[idx])\n",
        "        except (ValueError, TypeError):\n",
        "            label = 0\n",
        "\n",
        "        full_text = self.instruction + text\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            full_text,\n",
        "            truncation=True,\n",
        "            padding=False,\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        item = {key: encoding[key].squeeze() for key in encoding.keys()}\n",
        "        item['labels'] = torch.tensor(label, dtype=torch.long)\n",
        "        return item\n",
        "\n",
        "\n",
        "def compute_metrics_binary(p):\n",
        "    logits = p.predictions\n",
        "    preds = np.argmax(logits, axis=1)\n",
        "    labels = p.label_ids\n",
        "\n",
        "    macro_f1 = f1_score(labels, preds, average='macro')\n",
        "    weighted_f1 = f1_score(labels, preds, average='weighted')\n",
        "    acc = accuracy_score(labels, preds)\n",
        "\n",
        "    return {\n",
        "        'f1_macro': macro_f1,\n",
        "        'f1_weighted': weighted_f1,\n",
        "        'accuracy': acc\n",
        "    }\n",
        "\n",
        "def train_and_evaluate_model(model_name, language, X_train, y_train, X_val, y_val, X_test, test_df, class_weights):\n",
        "    \"\"\"\n",
        "    Train and evaluate a single model\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"Training: {model_name} on {language.upper()}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    start_time = datetime.now()\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    train_dataset = PolarizationDatasetBinary(X_train.tolist(), y_train.tolist(), tokenizer, MAX_LENGTH)\n",
        "    val_dataset = PolarizationDatasetBinary(X_val.tolist(), y_val.tolist(), tokenizer, MAX_LENGTH)\n",
        "\n",
        "    model = LabelAwareDebertaBinary(\n",
        "        model_name=model_name,\n",
        "        num_labels=2,\n",
        "        dropout_rate=DROPOUT_RATE\n",
        "    ).to(device)\n",
        "\n",
        "    model.class_weights = class_weights\n",
        "\n",
        "    # Training arguments\n",
        "    total_steps = (len(train_dataset) // BATCH_SIZE) * NUM_EPOCHS\n",
        "    warmup_steps = int(0.1 * total_steps)\n",
        "\n",
        "    model_short_name = model_name.split('/')[-1]\n",
        "    output_dir = f\"./output_{language}_{model_short_name}_subtask1\"\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=output_dir,\n",
        "        num_train_epochs=NUM_EPOCHS,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        per_device_train_batch_size=BATCH_SIZE,\n",
        "        per_device_eval_batch_size=BATCH_SIZE,\n",
        "        weight_decay=0.01,\n",
        "        warmup_steps=warmup_steps,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        save_total_limit=1,\n",
        "        load_best_model_at_end=True,\n",
        "        metric_for_best_model=\"f1_macro\",\n",
        "        greater_is_better=True,\n",
        "        logging_steps=50,\n",
        "        report_to=\"none\",\n",
        "        seed=42\n",
        "    )\n",
        "\n",
        "    # Trainer\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        train_dataset=train_dataset,\n",
        "        eval_dataset=val_dataset,\n",
        "        compute_metrics=compute_metrics_binary,\n",
        "        data_collator=DataCollatorWithPadding(tokenizer)\n",
        "    )\n",
        "\n",
        "    print(f\"\\nStarting training...\")\n",
        "    trainer.train()\n",
        "\n",
        "    print(f\"\\nEvaluating on validation set...\")\n",
        "    val_results = trainer.evaluate(val_dataset)\n",
        "\n",
        "    print(f\"\\nValidation Results:\")\n",
        "    print(f\"   Macro F1:    {val_results['eval_f1_macro']:.4f}\")\n",
        "    print(f\"   Weighted F1: {val_results['eval_f1_weighted']:.4f}\")\n",
        "    print(f\"   Accuracy:    {val_results['eval_accuracy']:.4f}\")\n",
        "\n",
        "\n",
        "    print(f\"\\nGenerating predictions on test set...\")\n",
        "\n",
        "    if 'polarization' in test_df.columns:\n",
        "        test_labels = test_df['polarization'].fillna(0).astype(int).tolist()\n",
        "    else:\n",
        "        test_labels = [0] * len(test_df)\n",
        "\n",
        "    test_dataset = PolarizationDatasetBinary(test_df['text'].tolist(), test_labels, tokenizer, MAX_LENGTH)\n",
        "\n",
        "    trainer.compute_metrics = None\n",
        "    prediction_output = trainer.predict(test_dataset)\n",
        "\n",
        "    logits = prediction_output.predictions\n",
        "    predictions = np.argmax(logits, axis=1)\n",
        "\n",
        "    count_0 = (predictions == 0).sum()\n",
        "    count_1 = (predictions == 1).sum()\n",
        "    print(f\"\\nPrediction Distribution:\")\n",
        "    print(f\"   Polarized (0):     {count_0} ({count_0/len(predictions)*100:.1f}%)\")\n",
        "    print(f\"   Not Polarized (1): {count_1} ({count_1/len(predictions)*100:.1f}%)\")\n",
        "\n",
        "    training_time = (datetime.now() - start_time).total_seconds() / 60\n",
        "    print(f\"\\nTraining time: {training_time:.2f} minutes\")\n",
        "\n",
        "    # Save predictions\n",
        "    os.makedirs(PREDICTIONS_DIR, exist_ok=True)\n",
        "\n",
        "    submission_df = pd.DataFrame()\n",
        "    if 'id' in test_df.columns:\n",
        "        submission_df['id'] = test_df['id']\n",
        "    elif 'ID' in test_df.columns:\n",
        "        submission_df['id'] = test_df['ID']\n",
        "\n",
        "    submission_df['polarization'] = predictions\n",
        "\n",
        "    pred_filename = f\"{PREDICTIONS_DIR}/predictions_{language}_{model_short_name}.csv\"\n",
        "    submission_df.to_csv(pred_filename, index=False)\n",
        "    print(f\"\\nPredictions saved to {pred_filename}\")\n",
        "\n",
        "    results = {\n",
        "        'timestamp': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
        "        'model_name': model_name,\n",
        "        'model_short_name': model_short_name,\n",
        "        'language': language,\n",
        "        'train_samples': len(X_train),\n",
        "        'val_samples': len(X_val),\n",
        "        'test_samples': len(X_test),\n",
        "        'val_f1_macro': val_results['eval_f1_macro'],\n",
        "        'val_f1_weighted': val_results['eval_f1_weighted'],\n",
        "        'val_accuracy': val_results['eval_accuracy'],\n",
        "        'pred_polarized': int(count_0),\n",
        "        'pred_not_polarized': int(count_1),\n",
        "        'training_time_minutes': training_time\n",
        "    }\n",
        "\n",
        "    return results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gn6i0e9lOqxM"
      },
      "outputs": [],
      "source": [
        "def main():\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"BINARY POLARIZATION CLASSIFICATION - SUBTASK 1\")\n",
        "    print(f\"Language: {LANGUAGE.upper()}\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    # Load data\n",
        "    print(f\"\\nLoading data for {LANGUAGE}...\")\n",
        "    train_path = f'./subtask1/train/{LANGUAGE}.csv'\n",
        "    dev_path = f'./subtask1/dev/{LANGUAGE}.csv'\n",
        "\n",
        "    try:\n",
        "        train_full = pd.read_csv(train_path)\n",
        "        test_df = pd.read_csv(dev_path)\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return\n",
        "\n",
        "    if 'polarization' not in train_full.columns:\n",
        "        print(\"Warning: 'polarization' column not found. Deriving from multilabel columns...\")\n",
        "        old_labels = ['gender/sexual', 'political', 'religious', 'racial/ethnic', 'other']\n",
        "        if set(old_labels).issubset(train_full.columns):\n",
        "            is_polarized = train_full[old_labels].sum(axis=1) > 0\n",
        "            train_full['polarization'] = np.where(is_polarized, 0, 1)\n",
        "        else:\n",
        "            raise ValueError(\"Input CSV must have 'polarization' column or multilabel columns.\")\n",
        "\n",
        "    X = train_full['text'].values\n",
        "    y = train_full['polarization'].values\n",
        "\n",
        "    print(f\"\\nCalculating class weights...\")\n",
        "    labels_unique, counts = np.unique(y, return_counts=True)\n",
        "    class_counts = dict(zip(labels_unique, counts))\n",
        "    print(f\"   Class distribution: {class_counts}\")\n",
        "\n",
        "    count_0 = class_counts.get(0, 0)\n",
        "    count_1 = class_counts.get(1, 0)\n",
        "    total = count_0 + count_1\n",
        "\n",
        "    weight_0 = (1 / count_0) * (total / 2.0)\n",
        "    weight_1 = (1 / count_1) * (total / 2.0)\n",
        "\n",
        "    class_weights = torch.tensor([weight_0, weight_1], dtype=torch.float).to(device)\n",
        "    print(f\"   Class weights: 0={weight_0:.2f}, 1={weight_1:.2f}\")\n",
        "\n",
        "    print(f\"\\nCreating train/val split...\")\n",
        "    X_train, X_val, y_train, y_val = train_test_split(\n",
        "        X, y, test_size=0.2, random_state=42, stratify=y\n",
        "    )\n",
        "    print(f\"   Train: {len(X_train)}, Val: {len(X_val)}\")\n",
        "\n",
        "    X_test = test_df['text'].values\n",
        "\n",
        "    models_to_test = MODELS_CONFIG.get(LANGUAGE, [])\n",
        "    if not models_to_test:\n",
        "        print(f\"No models configured for language: {LANGUAGE}\")\n",
        "        return\n",
        "\n",
        "    print(f\"\\nModels to test: {len(models_to_test)}\")\n",
        "    for i, model in enumerate(models_to_test, 1):\n",
        "        print(f\"   {i}. {model}\")\n",
        "\n",
        "    all_results = []\n",
        "    for model_name in models_to_test:\n",
        "        try:\n",
        "            results = train_and_evaluate_model(\n",
        "                model_name, LANGUAGE,\n",
        "                X_train, y_train,\n",
        "                X_val, y_val,\n",
        "                X_test,\n",
        "                test_df,\n",
        "                class_weights\n",
        "            )\n",
        "            all_results.append(results)\n",
        "\n",
        "            results_df = pd.DataFrame(all_results)\n",
        "\n",
        "            if os.path.exists(RESULTS_CSV):\n",
        "                existing_df = pd.read_csv(RESULTS_CSV)\n",
        "                results_df = pd.concat([existing_df, results_df], ignore_index=True)\n",
        "\n",
        "            results_df.to_csv(RESULTS_CSV, index=False)\n",
        "            print(f\"\\nResults updated in {RESULTS_CSV}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"\\nError training {model_name}: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            continue\n",
        "\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"TRAINING COMPLETE\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"\\nSuccessfully trained {len(all_results)} out of {len(models_to_test)} models\")\n",
        "    print(f\"Results saved to: {RESULTS_CSV}\")\n",
        "    print(f\"Predictions saved to: {PREDICTIONS_DIR}/\")\n",
        "\n",
        "    if all_results:\n",
        "        print(f\"\\nBest Model (by Validation Macro F1):\")\n",
        "        best_model = max(all_results, key=lambda x: x['val_f1_macro'])\n",
        "        print(f\"   Model: {best_model['model_short_name']}\")\n",
        "        print(f\"   Val Macro F1: {best_model['val_f1_macro']:.4f}\")\n",
        "        print(f\"   Training Time: {best_model['training_time_minutes']:.1f} min\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ThVCJdWCQqYU",
        "outputId": "1ee3398f-1170-4df5-9f2e-23cfb3f05097"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "BINARY POLARIZATION CLASSIFICATION - SUBTASK 1\n",
            "Language: SWA\n",
            "================================================================================\n",
            "\n",
            "Loading data for swa...\n",
            "\n",
            "Calculating class weights...\n",
            "   Class distribution: {np.int64(0): np.int64(3487), np.int64(1): np.int64(3504)}\n",
            "   Class weights: 0=1.00, 1=1.00\n",
            "\n",
            "Creating train/val split...\n",
            "   Train: 5592, Val: 1399\n",
            "\n",
            "Models to test: 2\n",
            "   1. cardiffnlp/twitter-roberta-base-hate-latest\n",
            "   2. Davlan/afro-xlmr-base\n",
            "\n",
            "================================================================================\n",
            "Training: cardiffnlp/twitter-roberta-base-hate-latest on SWA\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at cardiffnlp/twitter-roberta-base-hate-latest and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1750' max='1750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1750/1750 11:52, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1 Macro</th>\n",
              "      <th>F1 Weighted</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.587800</td>\n",
              "      <td>0.508916</td>\n",
              "      <td>0.759420</td>\n",
              "      <td>0.759353</td>\n",
              "      <td>0.763402</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.506700</td>\n",
              "      <td>0.488754</td>\n",
              "      <td>0.763168</td>\n",
              "      <td>0.763184</td>\n",
              "      <td>0.763402</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.433700</td>\n",
              "      <td>0.482230</td>\n",
              "      <td>0.769830</td>\n",
              "      <td>0.769827</td>\n",
              "      <td>0.769836</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.319400</td>\n",
              "      <td>0.539000</td>\n",
              "      <td>0.772633</td>\n",
              "      <td>0.772641</td>\n",
              "      <td>0.772695</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.326400</td>\n",
              "      <td>0.552880</td>\n",
              "      <td>0.780482</td>\n",
              "      <td>0.780490</td>\n",
              "      <td>0.780558</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating on validation set...\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Results:\n",
            "   Macro F1:    0.7805\n",
            "   Weighted F1: 0.7805\n",
            "   Accuracy:    0.7806\n",
            "\n",
            "Generating predictions on test set...\n",
            "\n",
            "Prediction Distribution:\n",
            "   Polarized (0):     157 (45.0%)\n",
            "   Not Polarized (1): 192 (55.0%)\n",
            "\n",
            "Training time: 12.08 minutes\n",
            "\n",
            "Predictions saved to predictions_subtask1_simple/predictions_swa_twitter-roberta-base-hate-latest.csv\n",
            "\n",
            "Results updated in results_subtask1_simple.csv\n",
            "\n",
            "================================================================================\n",
            "Training: Davlan/afro-xlmr-base on SWA\n",
            "================================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of XLMRobertaModel were not initialized from the model checkpoint at Davlan/afro-xlmr-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Starting training...\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='1750' max='1750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [1750/1750 16:01, Epoch 5/5]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1 Macro</th>\n",
              "      <th>F1 Weighted</th>\n",
              "      <th>Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.498400</td>\n",
              "      <td>0.491822</td>\n",
              "      <td>0.770936</td>\n",
              "      <td>0.770918</td>\n",
              "      <td>0.771265</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.454900</td>\n",
              "      <td>0.460894</td>\n",
              "      <td>0.785411</td>\n",
              "      <td>0.785423</td>\n",
              "      <td>0.785561</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.380400</td>\n",
              "      <td>0.463838</td>\n",
              "      <td>0.788093</td>\n",
              "      <td>0.788111</td>\n",
              "      <td>0.788420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.311100</td>\n",
              "      <td>0.512312</td>\n",
              "      <td>0.793271</td>\n",
              "      <td>0.793283</td>\n",
              "      <td>0.793424</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.270300</td>\n",
              "      <td>0.601291</td>\n",
              "      <td>0.793219</td>\n",
              "      <td>0.793233</td>\n",
              "      <td>0.793424</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Evaluating on validation set...\n"
          ]
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Validation Results:\n",
            "   Macro F1:    0.7933\n",
            "   Weighted F1: 0.7933\n",
            "   Accuracy:    0.7934\n",
            "\n",
            "Generating predictions on test set...\n",
            "\n",
            "Prediction Distribution:\n",
            "   Polarized (0):     161 (46.1%)\n",
            "   Not Polarized (1): 188 (53.9%)\n",
            "\n",
            "Training time: 16.28 minutes\n",
            "\n",
            "Predictions saved to predictions_subtask1_simple/predictions_swa_afro-xlmr-base.csv\n",
            "\n",
            "Results updated in results_subtask1_simple.csv\n",
            "\n",
            "================================================================================\n",
            "TRAINING COMPLETE\n",
            "================================================================================\n",
            "\n",
            "Successfully trained 2 out of 2 models\n",
            "Results saved to: results_subtask1_simple.csv\n",
            "Predictions saved to: predictions_subtask1_simple/\n",
            "\n",
            "Best Model (by Validation Macro F1):\n",
            "   Model: afro-xlmr-base\n",
            "   Val Macro F1: 0.7933\n",
            "   Training Time: 16.3 min\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9VtOVKn0Qtsy"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
